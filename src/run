#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import subprocess
from datetime import datetime, timedelta

MATOMO_API_ENDP = os.environ['MATOMO_API_ENDP']
MODES = json.loads(os.getenv('MODES'))
OUTDIR = os.environ['OUTDIR']
CACHEDIR = os.environ['CACHEDIR']
CUTOFF_DAYS = int(os.environ['CUTOFF_DAYS'])
REDO_OUTPUT = str(os.getenv('REDO_OUTPUT')).lower() == 'true'


def outstream(filename):
    return open(filename, "w")


def instream(filename):
    return open(filename, "r")


def get_cache_filename(odir, name, d):
    return f'{odir}/matomo_{d}_{name}.json'


def get_output_filename(odir, date):
    return f'{odir}/matomo_{date}.csv'


def generate_dates(date, cutoff_date):
    epoch = datetime.strptime(date, '%Y-%m-%d')
    cutoff = datetime.strptime(cutoff_date, '%Y-%m-%d')

    i = epoch
    while i < cutoff:
        yield i
        i += timedelta(1, 0, 0)


def get_cutoff_date(cutoff_days):
    cutoff = datetime.now() - timedelta(cutoff_days, 0, 0)
    return cutoff.strftime('%Y-%m-%d')


def get_earliest(dates):
    earliest = dates[0]
    for date in dates:
        date_time = datetime.strptime(date, '%Y-%m-%d')
        earliest_time = datetime.strptime(earliest, '%Y-%m-%d')
        try:
            assert date_time > earliest_time
        except AssertionError:
            earliest = date
    return earliest


def exists_and_not_empty(filename):
    try:
        return os.path.getsize(filename) > 0
    except (AssertionError, OSError):
        return False


def old_or_empty(filename, cutoff):
    cutoff = datetime.now() - timedelta(days=cutoff)
    try:
        size = os.path.getsize(filename)
        time = datetime.fromtimestamp(os.path.getctime(filename))
        assert size > 0 and time > cutoff
    except (AssertionError, OSError):
        return True
    return False


def compile_config(config_list):
    vals = []
    for c in config_list:
        vals.append('--' + c['name'])
        vals.append(c['value'])
    return vals


def cache_stats(outputstream, endpoint, date, config):
    cmd = ['./retrieve_stats',
           '--api-endpoint', endpoint,
           '--date', date] + config
    subprocess.call(cmd, stdout=outputstream)


def resolve_cache(output_stream, input_stream, measure,
                  prefix, date, regex, headers):
    regexes = compile_config([{'name': 'regex', 'value': r} for r in regex])
    add_headers = ['--add-headers'] if headers else []
    cmd = ['./resolve_reports',
           '--measure', measure,
           '--prefix', prefix,
           '--date', date] + regexes + add_headers
    subprocess.call(cmd, stdout=output_stream, stdin=input_stream)


def run():
    cutoff_date = get_cutoff_date(CUTOFF_DAYS)
    # cache Matomo API responses for all MODES
    for m in MODES:
        config = compile_config(m['config'])
        for day in generate_dates(m['startDate'], cutoff_date):
            date = day.strftime('%Y-%m-%d')
            cache_file = get_cache_filename(CACHEDIR, m['name'], date)
            if not exists_and_not_empty(cache_file):
                output = outstream(cache_file)
                cache_stats(output, MATOMO_API_ENDP, date, config)

    # now we standarise reports and store them in each output CSV
    earliest_date = get_earliest([m['startDate'] for m in MODES])
    for day in generate_dates(earliest_date, cutoff_date):
        date = day.strftime('%Y-%m-%d')
        out_file = get_output_filename(OUTDIR, date)

        # continue if output file already exists, unless redoing them
        if exists_and_not_empty(out_file) and not REDO_OUTPUT:
            continue

        i = 0
        output = outstream(out_file)
        for m in MODES:
            cache_file = get_cache_filename(CACHEDIR, m['name'], date)
            # at this point all *relevant* cache files must exists
            if not exists_and_not_empty(cache_file):
                continue
            inputs = instream(cache_file)
            regex = m['regex'] if 'regex' in m else []
            headers = i == 0  # only include headers in first iteration
            i += 1
            resolve_cache(output, inputs, m['measure'],
                          m['prefix'], date, regex, headers)


if __name__ == '__main__':
    run()
